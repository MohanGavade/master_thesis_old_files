{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "folder_path = \"path_to_my_folder\"\n",
    "\n",
    "pickle_files = [f for f in os.listdir(folder_path) if f.endswith('.p')]\n",
    "\n",
    "print(\"files found in the folder\")\n",
    "print(pickle_files)\n",
    "\n",
    "\n",
    "def load_pickle(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "all_data = {}\n",
    "\n",
    "for file in pickle_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    all_data[file] = load_pickle(file_path)\n",
    "\n",
    "print(\"\\nLoaded pickle files:\")\n",
    "for file_name in all_data:\n",
    "    print(f\"File: {file_name}, Keys: {list(all_data[file_name].keys())}\"\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\"rcs\", \"distance\", \"angleAzimuth\", \"angleElevation\", \"radialVelocity\", \"radialVelocityDomainMax\", \"SNR\", 'yawrate', 'egospeed' ]\n",
    "label_column = [\"orientation\", \"x\", \"y\", \" width_edge_mean\", \"length_edge_mean\"]\n",
    "\n",
    "\n",
    "def clean_files(data):\n",
    "\n",
    "    cleaned_features = data[feature_columns]\n",
    "    cleaned_labels = data[label_column]\n",
    "    return cleaned_features, cleaned_labels\n",
    "\n",
    "first_file_name = list(all_data.keys())[0]\n",
    "print(f\"processing file: {first_file_name}\")\n",
    "\n",
    "raw_data = all_data[first_file_name]\n",
    "x_raw, y_raw = clean_files(pd.DataFrame(raw_data))\n",
    "\n",
    "print(\"cleaned features (first 2 rows:)\")\n",
    "print(x_raw.head(2))\n",
    "print(\"\\ncleaned labels (first 2 rows):\")\n",
    "print(y_raw.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Data Types and Convert to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_numeric(x, y):\n",
    "\n",
    "    for col in x.columns:\n",
    "        x[col] =x[col].apply(lambda v: np.array(v, dtype=np.float32)) if isinstance(v, list) else v)\n",
    "\n",
    "    y[label_column] = y[label_column].apply(lambda v: np.array(v, dtype=np.float32) if isinstance(v, list) else v)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "x_numeric, y_numeric = convert_to_numeric(x_raw, y_raw)\n",
    "\n",
    "print(\"\\nFeatures after conversion to numeric (first 2 rows):\")\n",
    "print(x_numeric.head(2))\n",
    "\n",
    "print(\"\\nlables after conversion to numeric (first 2 rows):\")\n",
    "print(y_numeric.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def normalize_features(x, feature_columns):\n",
    "\n",
    "    for col in feature_columns:\n",
    "        feature_values = x[col]\n",
    "        mean = np.mean(np.concatenate(feature_values))\n",
    "        std = np.std(np.concatenate(feature_values))\n",
    "        x[col] = x[col].apply(lambda v: (v-mean) / (std + 1e-8))\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(data, fixed_length=None):\n",
    "    tensor_data = [torch.tensor(seq, dtype = torch.float32) for seq in data]\n",
    "    if fixed_length:\n",
    "        padded_data = torch.stack([\n",
    "            torch.cat([seq, torch.zeros(fixed_length - seq.shape[0], *seq.shape[1:], dtype=torch.float32)])\n",
    "            if seq.shape[0] < fixed_length else seq[:fixed_length]\n",
    "            for seq in tensor_data\n",
    "        ])\n",
    "    else:\n",
    "        padded_data = pad_sequence(tensor_data, batch_first=True)\n",
    "    return padded_data\n",
    "\n",
    "x_normalized = normalize_features(x_numeric, feature_columns)\n",
    "\n",
    "x_combined = [\n",
    "    np.column_stack([\n",
    "        row[\"rcs\"], row[\"distance\"], row[\"angleAzimuth\"], row[\"angleElevation\"], row[\"radialVelocity\"]\n",
    "    ])\n",
    "    for _, row in x_normalized.iterrows()\n",
    "]\n",
    "\n",
    "x_padded = pad_sequences(x_combined)\n",
    "y_padded = pad_sequences(y_numeric[label_column])\n",
    "\n",
    "print(\"\\nShape of x?padde:\", x_padded.shape)\n",
    "print(\"\\nShape of y_padded:\", y_padded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide data into Train, Validate, test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "file_names = list(all_data.keys())\n",
    "train_files, temp_files = train_test_split(file_names, train_size=0.8, random_state=42)\n",
    "val_files, test_files = train_test_split(temp_files, train_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"training files: {train_files}\")\n",
    "print(f\"valiadation files: {val_files}\")\n",
    "print(f\"testing files: {test_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess each file based on its split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_file(data, feature_columns, label_column, fixed_length=50):\n",
    "     \n",
    "     x_raw= data[feature_columns]\n",
    "     y_raw= data[label_column]\n",
    "\n",
    "     x_numeric, y_numeric = convert_to_numeric(x_raw, y_raw)\n",
    "\n",
    "     x_normalized= normalize_features(x_numeric, feature_columns)\n",
    "\n",
    "     x_combined = [\n",
    "          np.column_stack([\n",
    "               row[\"rcs\"], row[\"distance\"], row[\"angleAzimuth\"], row[\"angleElevation\"], row[\"radialVeocity\"]\n",
    "          ])\n",
    "          for _, row in x_normalized.iterrows()\n",
    "     ]\n",
    "\n",
    "     x_padded = pad_sequences(x_combined, fixed_length=fixed_length)\n",
    "     y_padded = pad_sequences(y_numeric[label_column], fixed_length=fixed_length)\n",
    "\n",
    "     return x_padded, y_padded\n",
    "\n",
    "\n",
    "processed_data_splits = {\"train\": [], \"val\":[], \"test\":[]}\n",
    "\n",
    "for file_name in file_names:\n",
    "     raw_data= pd.Dataframe(all_data[file_name])\n",
    "     x_padded, y_padded = preprocess_file(raw_data, feature_columns, label_column, fixed_length=50)\n",
    "\n",
    "     if file_name in train_files:\n",
    "        processed_data_splits[\"train\"].append((x_padded, y_padded))\n",
    "     elif file_name in val_files:\n",
    "        processed_data_splits[\"val\"].append(x_padded, y_padded))\n",
    "     elif file_name in test_files:\n",
    "        processed_data_splits[\"test\"].append((x_padded, y_padded))\n",
    "print(\"files divided into train, val, and test splits\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combine data within each split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_splits(split_data):\n",
    "\n",
    "    x_combined = torch.cat([x for x, _in split_data], dim=0)\n",
    "    y_combined = torch.cat([y for _, y in split_data], dim=0)\n",
    "\n",
    "    return x_combined, y_combined\n",
    "\n",
    "x_train_combined, y_train_combined = combine_splits(processed_data_splits[\"train\"])\n",
    "x_val_combined, y_val_combined = combine_splits(processed_data_splits[\"val\"])\n",
    "x_test_combined, y_test_combined = combine_splits(processed_data_splits[\"test\"])\n",
    "\n",
    "print(\"combined splits ready for training:\")\n",
    "print(f\"x_train_shape: {x_train_combined.shape},y_train shape: {y_train_combined.shape}\")\n",
    "print(f\"x_val shape: {x_valcombined.shape}, y_val shape: {y_val_combined.shape}\")\n",
    "print(f\"x_test shape: {x_test_combined.shape}, y_test shape: {y:test_combined.shape}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
